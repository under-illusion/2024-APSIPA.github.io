<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>2024 APSIPA China-Japan Joint Symposium on Speech and Language Processing</title>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/xlsx/0.17.0/xlsx.full.min.js"></script>
    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="robots" content="all,follow">
    <!-- Bootstrap CSS-->
    <link rel="stylesheet" href="vendor/bootstrap/css/bootstrap.min.css">
    <!-- Google fonts-->
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,700">
    <!-- Icon fonts-->
    <link rel="stylesheet" href="css/pe-icon-7-stroke.css">
    <!-- Lightbox CSS-->
    <link rel="stylesheet" href="vendor/lightbox2/css/lightbox.min.css">
    <!-- theme stylesheet-->
    <link rel="stylesheet" href="css/style.default.css" id="theme-stylesheet">
    <!-- Custom stylesheet - for your changes-->
    <link rel="stylesheet" href="css/custom.css">
    <!-- Favicon-->
    <link rel="shortcut icon" href="img/favicon.png">
    <!-- Tweaks for older IEs--><!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
        <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script><![endif]-->
  </head>
  <body>
    <!-- navbar-->
    <header class="header">
      <nav class="navbar navbar-expand-lg py-lg-0">
          <div id="navbarSupportedContent" class="collapse navbar-collapse">
            <ul class="navbar-nav ml-auto d-lg-flex align-items-lg-center">
              <li class="nav-item"><a href="index.html" class="nav-link font-weight-bold text-uppercase px-lg-3 py-lg-4 ">Conference Information</a>
              </li>
              <li class="nav-item"><a href="program.html" class="nav-link font-weight-bold text-uppercase px-lg-3 py-lg-4">Program</a>
              </li>
              <li class="nav-item"><a href="hotel.html" class="nav-link font-weight-bold text-uppercase px-lg-3 py-lg-4">Recommended Hotels</a>
              </li>
              <li class="nav-item"><a href="attraction.html" class="nav-link font-weight-bold text-uppercase px-lg-3 py-lg-4">Recommended Attractions</a>
              </li>
              <li class="nav-item"><a href="poster.html" class="nav-link font-weight-bold text-uppercase px-lg-3 py-lg-4 active">Poster Information</a>
              </li>
              <li class="nav-item"><a href="other.html" class="nav-link font-weight-bold text-uppercase px-lg-3 py-lg-4">Other Information</a>
              </li>
            </ul>
          </div>
        </div>
      </nav>
    </header>

    <section style="background: url(img/Picture16.png)" class="py-5 bg-cover bg-center">
      <div class="hero-overlay"></div>
      <div class="container py-5 text-white text-center">
        <h1 class="text-shadow hero-heading">2024 APSIPA China-Japan Joint Symposium on Speech and Language Processing</h1>
        <p class="text-shadow lead my-4" style="color: white;"><img src="img/time.png" style="height: 20px;"></img> Date: September 24-25, 2024</p>
        <p class="text-shadow lead my-4" style="color: white;"><img src="img/loaction.png" style="height: 20px;"></img>Venue: 55B-204, College of Intelligence and Computing, Peiyangyuan Campus, Tianjin University, China</p>
      </div>
    </section><br><br>

    <div class="content">
      <h2 class="mb-4">Call for Poster Submission</h2>
      <div style="text-align: left;">
      <p class="lead" style="color: black;">
        &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This conference does not accept submissions or publish proceedings. We
        welcome participants to share their research results through poster presentations
        (60 minutes). Please send your title, abstract, and personal information to the
        conference organizing committee’s email (wangxiaobao@tju.edu.cn) by
        September 8, 2024. Acceptance notifications will be sent via email. <br>
        &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;We welcome displays on various topics related to speech information processing. The poster
        should not exceed the size of 118cm x 84cm. Boards will be provided at the venue,
        and you can simply put up your poster upon arrival. The format is flexible, but
        the content must be in English.
        Poster Venue Location
      </p>
      <div style="text-align: center;">
        <img src="./img/Picture8.jpg" ></img><br>
        <span style="font-weight: bold;">Poster Venue Location</span>
      </div>
      <br><br>

    <div style="width: 80%; padding-left: 10%;">

        <h1 class="text-2xl font-bold mb-4">The currently poster</h1>  

        <table class="min-w-full bg-white border border-gray-300 rounded-lg shadow-md">  
            <thead>  
                <tr class="bg-gray-200 text-gray-600 uppercase text-sm leading-normal">  
                    <th class="py-3 px-6">Name</th>  
                    <th class="py-3 px-6">Affiliation</th>  
                    <th class="py-3 px-6">Topic</th>  
                </tr>  
            </thead>  
            <tbody class="text-gray-600 text-sm font-light">  
                <tr class="border-b border-gray-300 hover:bg-gray-100">  
                    <td class="py-3 px-6">Zekun YANG</td>  
                    <td class="py-3 px-6">Nagoya University</td>  
                    <td class="py-3 px-6">Multi-Modal Video Summarization Based on Two-Stage Fusion of Audio, Visual, and Recognized Text Information</td>  
                </tr>  
                <tr class="border-b border-gray-300 hover:bg-gray-100">  
                    <td class="py-3 px-6">Fengji LI</td>  
                    <td class="py-3 px-6">Nagoya University & Beihang University</td>  
                    <td class="py-3 px-6">Mandarin Speech Reconstruction from Ultrasound Tongue Images based on Generative Adversarial Networks</td>  
                </tr>  
                <tr class="border-b border-gray-300 hover:bg-gray-100">  
                    <td class="py-3 px-6">Shaowen CHEN</td>  
                    <td class="py-3 px-6">Nagoya University</td>  
                    <td class="py-3 px-6">QHM-GAN: Neural Vocoder based on Quasi-Harmonic Modeling</td>  
                </tr>  
                <tr class="border-b border-gray-300 hover:bg-gray-100">  
                    <td class="py-3 px-6">Xiaohan Shi</td>  
                    <td class="py-3 px-6">Nagoya University</td>  
                    <td class="py-3 px-6">Speech emotion prediction towards development of emotion-aware dialogue systems</td>  
                </tr>  
                <tr class="border-b border-gray-300 hover:bg-gray-100">  
                    <td class="py-3 px-6">Jingyi FENG</td>  
                    <td class="py-3 px-6">Nagoya University</td>  
                    <td class="py-3 px-6">Robustness of TTS Models Trained on Noisy Transcriptions</td>  
                </tr>  
                <tr class="border-b border-gray-300 hover:bg-gray-100">  
                    <td class="py-3 px-6">Rui Wang</td>  
                    <td class="py-3 px-6">Nagoya University</td>  
                    <td class="py-3 px-6">Direction-aware target speaker extraction under noisy underdetermined conditions</td>  
                </tr>  
                <tr class="border-b border-gray-300 hover:bg-gray-100">  
                    <td class="py-3 px-6">Hao Shi</td>  
                    <td class="py-3 px-6">Kyoto University</td>  
                    <td class="py-3 px-6">Speech Enhancement using spectrogram feature fusion for noise robust speech recognition</td>  
                </tr>  
                <tr class="border-b border-gray-300 hover:bg-gray-100">  
                    <td class="py-3 px-6">Yahui Fu</td>  
                    <td class="py-3 px-6">Kyoto University</td>  
                    <td class="py-3 px-6">Dialogue Comprehension and Personalization for Empathetic Response Generation</td>  
                </tr>  
                <tr class="border-b border-gray-300 hover:bg-gray-100">  
                    <td class="py-3 px-6">Mewlude Nijat</td>  
                    <td class="py-3 px-6">Xinjiang University</td>  
                    <td class="py-3 px-6">UY/CH-CHILD -- A Public Chinese L2 Speech Database of Uyghur Children</td>  
                </tr>  
                <tr class="border-b border-gray-300 hover:bg-gray-100">  
                    <td class="py-3 px-6">Yikang Wang</td>  
                    <td class="py-3 px-6">University of Yamanashi</td>  
                    <td class="py-3 px-6">A Study of Guided Masking Data Augmentation for Deepfake Speech Detection</td>  
                </tr>  
                <tr class="border-b border-gray-300 hover:bg-gray-100">  
                    <td class="py-3 px-6">Yu-Fei Shi</td>  
                    <td class="py-3 px-6">The University of Science and Technology of China</td>  
                    <td class="py-3 px-6">Pitch-and-Spectrum-Aware Singing Quality Assessment with Bias Correction and Model Fusion</td>  
                </tr>  
                <tr class="border-b border-gray-300 hover:bg-gray-100">  
                    <td class="py-3 px-6">Yuta Kamiya</td>  
                    <td class="py-3 px-6">Shizuoka University</td>  
                    <td class="py-3 px-6">The construction and comparative analysis of a nationwide regional dialect language model and identification model using the Corpus of Japanese Dialects.</td>  
                </tr>  
                <tr class="border-b border-gray-300 hover:bg-gray-100">  
                    <td class="py-3 px-6">Haopeng Geng</td>  
                    <td class="py-3 px-6">The University of Tokyo</td>  
                    <td class="py-3 px-6">A Pilot Study of Applying Sequence-to-Sequence Voice Conversion to Evaluate the Intelligibility of L2 Speech Using a Native Speaker’s Shadowings</td>  
                </tr>  
                <tr class="border-b border-gray-300 hover:bg-gray-100">  
                    <td class="py-3 px-6">Rui Wang</td>  
                    <td class="py-3 px-6">University of Science and Technology of China</td>  
                    <td class="py-3 px-6">Asynchronous Voice Anonymization Using Adversarial Perturbation On Speaker Embedding</td>  
                </tr>  
                <tr class="border-b border-gray-300 hover:bg-gray-100">  
                    <td class="py-3 px-6">Nobuaki Minematsu</td>  
                    <td class="py-3 px-6">The University of Tokyo</td>  
                    <td class="py-3 px-6">Measurement of listening behaviors of learners and raters and its application for aural/oral L2 training</td>  
                </tr>  
                <tr class="border-b border-gray-300 hover:bg-gray-100">  
                    <td class="py-3 px-6">Chenda li</td>  
                    <td class="py-3 px-6">Shanghai Jiao Tong University</td>  
                    <td class="py-3 px-6">Diffusion-based Generative Modeling with Discriminative Guidance for Streamable Speech Enhancement</td>  
                </tr>  
                <tr class="border-b border-gray-300 hover:bg-gray-100">  
                    <td class="py-3 px-6">MIKAWA Tamon, FUJII Yasuhisa, WAKABAYASHI Yukoh, OHTA Kengo, NISHIMURA Ryota, KITAOKA Norihide</td>  
                    <td class="py-3 px-6">Toyohashi University of Technology</td>  
                    <td class="py-3 px-6">Listener's Head Motion Generation Responding to User's Speech and Head Movements
                      Absract</td>  
                </tr>  
                <tr class="border-b border-gray-300 hover:bg-gray-100">  
                    <td class="py-3 px-6">Keigo Hojo, Yukoh Wakabayashi, Kengo Ohta, Atsunori Ogawa, Norihide Kitaoka</td>  
                    <td class="py-3 px-6">Toyohashi University of Technology</td>  
                    <td class="py-3 px-6">Improving the performance of CTC-based ASR using attention-based CTC loss</td>  
                </tr>  
                <tr class="border-b border-gray-300 hover:bg-gray-100">  
                    <td class="py-3 px-6">Tatsunari Takagi, Yukoh Wakabayashi, Atsunori Ogawa, Norihide Kitaoka</td>  
                    <td class="py-3 px-6">Toyohashi University of Technology</td>  
                    <td class="py-3 px-6">Text-only Domain Adaptation for CTC-based Speech Recognition through Substitution of Implicit Linguistic Information in the Search Space</td>  
                </tr>  
                <tr class="border-b border-gray-300 hover:bg-gray-100">  
                    <td class="py-3 px-6">Wei Wang</td>  
                    <td class="py-3 px-6">Shanghai Jiao Tong University</td>  
                    <td class="py-3 px-6">Advancing Non-intrusive Suppression on Enhancement Distortion for Noise Robust ASR</td>  
                </tr>  
                <tr class="border-b border-gray-300 hover:bg-gray-100">  
                  <td class="py-3 px-6"> Jiaming Zhou</td>  
                  <td class="py-3 px-6">Nankai University</td>  
                  <td class="py-3 px-6">kNN-CTC: Enhancing ASR via Retrieval of CTC Pseudo Labels</td>  
              </tr>  
              <tr class="border-b border-gray-300 hover:bg-gray-100">  
                  <td class="py-3 px-6">Xu Zhang</td>  
                  <td class="py-3 px-6">Beijing University of Technology</td>  
                  <td class="py-3 px-6">Iteratively Refined Multi-Channel Speech Separation</td>  
              </tr> 
              <tr class="border-b border-gray-300 hover:bg-gray-100">  
                  <td class="py-3 px-6">Xue Yang</td>  
                  <td class="py-3 px-6">Beijing University of Technology</td>  
                  <td class="py-3 px-6">Coarse-to-Fine Target Speaker Extraction Based on Contextual Information Exploitation</td>  
              </tr> 
            </tbody>  
        </table>  
    </div>

    <footer style="background: #eee;" class="pt-5">
      <div class="bg-dark py-4">    
        <div class="container" style="text-align: center;">
            <p class="small text-gray mb-0">Tianjin University</p>
        </div>
      </div>
    </footer>
    <!-- JavaScript files-->
    <script src="vendor/jquery/jquery.min.js"></script>
    <script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
    <script src="vendor/lightbox2/js/lightbox.min.js"></script>
    <script src="js/front.js"></script>
    <!-- FontAwesome CSS - loading as last, so it doesn't block rendering-->
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.7.1/css/all.css" integrity="sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr" crossorigin="anonymous">
  </body>
</html>
